---
title: "MAST90138 Assignment 2"
author: "Haonan Zhong"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Question 1a
```{r}
wheat <- read.csv("Wheat data.txt", sep = "", header = F)
X <- scale(wheat[, -c(8)], scale = FALSE)
PCX <- prcomp(X, retx = TRUE)
(lambda <- PCX$sdev^2)
(gamma <- PCX$rotation)
(fracvar <- lambda/sum(lambda))
```
As we can see from the output, the first and the second principle components was able to explain about 82.9% and 16.4% of the variability of the data, respectively. And the third component explains around 0.57% of variances. Whilst, the rest of the PCs only explains a tiny portion of variances.

```{r}
(cumuprop <- cumsum(lambda)/sum(lambda))
screeplot(PCX, type = "line")
```
Visually, one can look for an elbow in the screeplot and stop there. In our case, we should keep the first three principle components, with the first three PCs we explain almost 100% of the variability of the data.

### Question 1b
According to the eigenvectors given in the question 1a, we have
$$Y_{1} = -0.884X_1 - 0.395X_2 - 0.004X_3 - 0.129X_4 - 0.111X_5 + 0.127X_6 - 0.129X_7$$
Thus, the first PC puts the most weight on the first variable $X_1$, which has an negative effect on PC1, followed by $X_2$ with a positive effect. On the other hand, $X_4$, $X_5$, $X_6$, and $X_7$ are quite similar in terms of contribution, whilst $X_3$ plays the smallest role in the construction of the first PC.

$$Y_2 = 0.101X_1 +0.056X_2-0.003X_3 + 0.031X_4 + 0.002X_5 + 0.989X_6 + 0.082X_7$$
Thus, $X_6$ plays the major role in the construction of the second PC with a positive effect, followed by $X_1$.

### Question 1c
```{r}
plot(x = PCX$x[,1], y = PCX$x[,2], col = wheat$V8, pch=5, xlab = 'PC1', ylab = 'PC2')
legend(-3, 4, horiz = TRUE, unique(wheat$V8), col=1:length(wheat$V8), pch=5)
```
Based on the scatter plot between the first two principle components, we can see that PC1 captured most of the variation driven by the different varieties of wheat, as it divides the data points into three distinct clusters, where black points indicates data point from group 1, red points indicates data point from group 2, and green points represents data point from group 3. On the other hand, no clear group separation can be seen from PC2, other than group 1 seems to have a low value of PC2 compare to the other two groups.

### Question 1d
```{r}
corr_matrix <- matrix(0, 7, 7)

for (j in 1:7) {
  for (k in 1:7) {
    corr_matrix[j, k] <- (gamma[j, k] * lambda[k]) / sqrt(cov(X)[j, j] * lambda[k])
  }
}
corr_matrix[, c(1, 2)]
corr_matrix[,1]^2 + corr_matrix[,2]^2
```

```{r}
plot(x = 0, y = 0, xlim = c(-2, 2), ylim = c(-1, 1), xlab = 'PC1', ylab = 'PC2',
     main = "correlations between the Xj's and PC1 and PC2")
for (i in 1:7) {
  arrows(0, 0, corr_matrix[i, 1], corr_matrix[i, 2])
  text(corr_matrix[i, 1] - 0.2, 
       corr_matrix[i, 2],
       paste("X", as.character(i)))
}

radius <- 1
theta <- seq(0, 2 * pi, length = 200)
lines(x = radius * cos(theta), y = radius * sin(theta))
```
As the plot above depicted, most of the variables, except for $X3$, are relatively close to the periphery of the circle, which indicates that they are strongly correlated with the first two PCs. Furthermore, PC1 is strongly negatively correlated with $X1$, $X2$, $X4$, $X5$ and $X7$. Whilst PC2 is highly positively correlated with $X6$.
```{r}
plot(x = PCX$x[,1], y = PCX$x[,2], col = wheat$V8, xlab = 'PC1', ylab = 'PC2')
legend(-3, 4, horiz = TRUE, unique(wheat$V8), col=1:length(wheat$V8), pch=1)
```
We also know that together the first two PCs explains a large portion of the variability of the data, therefore we could also use the direction of the arrow in conjunction with the scatter plot of the first two PCs to learn the effect of these variables on individuals. In particular, group 2 tends to have negative values on PC1, while group 1 tend to have a value centering around 0 and group 3 tend to have large positive values on PC1. Contrarily, group 3 tend to have slightly larger values on PC2, followed by group 2, then group 1.

As we mentioned above, PC1 is strongly negatively correlated with $X1$, $X2$, $X4$, $X5$ and $X7$.Therefore, when a group has a low value of PC1, we expect it tend to go together with large values of $X1$, $X2$, $X4$, $X5$ and $X7$, and vice versa. In addition, given that PC2 is highly positively correlated with $X6$, we expect groups with large value on PC2 will also have a large value on $X6$. We can see that these are indeed the case in the scatter plot below. 

```{r}
pairs(X, col = c(1,2,3)[wheat$V8])
```

### Question 2a
Given that we are computing the orthogonal factor model with single factor $k=1$ for $\Sigma$. Therefore, we have $\Sigma = QQ^T+\Psi$, where
$$Q = \begin{bmatrix} q_1 \\ q_2 \\ q_3 \end{bmatrix} \; and \; \Psi = \begin{bmatrix} \psi_{11}&0&0 \\ 0&\psi_{22}&0 \\ 0&0&\psi_{33} \end{bmatrix}$$
$$\Sigma = \begin{bmatrix} 1&0.9&0.7 \\ 0.9&1&0.4 \\ 0.7&0.4&1 \end{bmatrix} = \begin{bmatrix} q_1^2+\psi_{11}&q_1q_2&q_1q_3 \\ q_1q_2&q_2^2+\psi_{22}&q_2q_3 \\ q_1q_3&q_2q_3&q_3^2+\psi_{33} \end{bmatrix}$$
Thus, we will have
$$q_1 = \sqrt{\frac{0.9\times0.7}{0.4}} = 1.25 \;\; q_2 = \sqrt{\frac{0.9\times0.4}{0.7}} = 0.72 \;\; q_3=\sqrt{\frac{0.7\times0.4}{0.9}}=0.56$$

$$\psi_{11} = 1-1.25^2=-0.57 \;\; \psi_{22} = 1-0.72^2 = 0.49 \;\; \psi_{33} = 1-0.56^2=0.69$$
Therefore, we have the following solution with,
$$Q = \begin{bmatrix} 1.25 \\ 0.72 \\ 0.56 \end{bmatrix} \;\; and \;\; \Psi = \begin{bmatrix} -0.57&0&0 \\ 0&0.49&0 \\ 0&0&0.69 \end{bmatrix}$$
The problem we have is that the $\psi_{11}$ is negative, and since $\Psi$ is the variance of the specific factors $U$ and cannot be negative, thus, this solution cannot be interpret as a factor analysis model.

### Question 2bi
```{r}
data("Harman23.cor")
Harman23.cor$n.obs
dim(Harman23.cor$cov)
```
Given that factor modeling is meant for dimension reduction. Therefore, to avoid over-parametrization, we generally requires
$$p(p+1)/2 \geqslant pq+p-q(q-1)/2$$
Therefore, the maximum number of factors we can fit is $p(p+1)/2$. In the case here, we have $p=8$, hence
$$8(8+1)/2\geqslant 8q+8-q(q-1)/2 $$
$$28 \geqslant 8q-q(q-1)/2 $$
$$56 \geqslant 17q-q^2$$
$$ 56 -17q+q^2 \geqslant 0 \\$$
Solving the above equation we have $q \leqslant 4.46887 \; or \; q\geqslant 12.53112$, thus, the maximum number of factor is 4.

### Question 2bii
```{r}
for (i in 1:4) {
  print(factanal(factors = i, covmat = Harman23.cor))
}
```
By testing the null hypothesis: $q$ is the number of factors for each allowed q, we can see that the p-value returned for $q=1$, $q=2$, $q=3$, and $q=4$ is 1.12e-116, 6.94e-11, 0.00184 and 0.0988, respectively. Among them, only $q=4$ has a p-value greater than the significance level of 0.05, hence we do not reject the null hypothesis and the factor model with $q=4$ has the best fit.
