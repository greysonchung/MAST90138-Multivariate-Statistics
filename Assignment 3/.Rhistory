knitr::opts_chunk$set(echo = TRUE)
library(MASS)
suppressMessages(library(pls))
XGtrain <- read.table("XGtrainRain.txt", header = TRUE, sep = ",")
XGtest <- read.table("XGtestRain.txt", header = TRUE, sep = ",")
# Fitting the quadratic discriminant model
# qda_model <- qda(G~., data = XGtrain)
# Fitting the logistic regression model
logistic <- glm(G~., data = XGtrain, family = binomial(link = "logit"))
# summary(logistic)
# Obtain the projection matrix for PCA
Xtrain <- scale(XGtrain[, -c(366)], scale=FALSE)
PCX <- prcomp(Xtrain, retx = T)
gamma <- PCX$rotation
# Manually re-compute the PC components
Y <- (Xtrain - matrix(rep(1, nrow(Xtrain)), nrow=nrow(Xtrain)) %*% colMeans(Xtrain)) %*% gamma
all(PCX$x == Y)
# Obtain the projection matrix for PLS
PLS <- plsr(G~., data = XGtrain)
phi <- PLS$projection
t <- Xtrain %*% phi
all(PLS$scores == t)
# Train QDA with PLS components
CV_error <- rep(0, 50)
Gtrain <- XGtrain[, 366]
for (q in 1:50) {
prediction <- c()
for (i in 1:dim(XGtrain)[1]) {
GDATACV <- Gtrain[-i]
YDATACV <- as.data.frame(PLS$scores[-i, 1:q])
QDA <- qda(GDATACV~., data = YDATACV)
new_data <- as.data.frame(t(PLS$scores[i, 1:q]))
colnames(new_data) <- colnames(YDATACV)
prediction[i] <- as.numeric(predict(QDA, newdata = new_data)$class) - 1
}
CV_error[q] <- sum(prediction != Gtrain)/dim(XGtrain)[1]
}
# Plotting the cross validation error
plot(c(1:50), CV_error, type = "l", xlab = "Values of q", ylab = "CV Error", main = "PLS CV Error")
which(CV_error == min(CV_error))
min(CV_error)
# Train QDA with PCA components
CV_error <- rep(0, 50)
for (q in 1:50) {
prediction <- c()
for (i in 1:dim(XGtrain)[1]) {
GDATACV <- Gtrain[-i]
YDATACV <- as.data.frame(PCX$x[-i, 1:q])
QDA <- qda(GDATACV~., data = YDATACV)
new_data <- as.data.frame(t(PCX$x[i, 1:q]))
colnames(new_data) <- colnames(YDATACV)
prediction[i] <- as.numeric(predict(QDA, newdata = new_data)$class) - 1
}
CV_error[q] <- sum(prediction != Gtrain)/dim(XGtrain)[1]
}
# Plotting the cross validation error
plot(c(1:50), CV_error, type = "l", xlab = "Values of q", ylab = "CV Error", main = "PCA CV Error")
which(CV_error == min(CV_error))
min(CV_error)
# Train logistic regression model with PLS
CV_error <- rep(0, 50)
for (q in 1:50) {
prediction <- c()
for (i in 1:dim(XGtrain)[1]) {
GDATACV <- Gtrain[-i]
YDATACV <- as.data.frame(PLS$scores[-i, 1:q])
suppressWarnings(logistic <- glm(GDATACV~., data = YDATACV, family = binomial(link = "logit")))
new_data <- as.data.frame(t(PLS$scores[i, 1:q]))
colnames(new_data) <- colnames(YDATACV)
prediction[i] <- ifelse(predict(logistic, newdata = new_data) > 0, 1, 0)
}
CV_error[q] <- sum(prediction != Gtrain)/dim(XGtrain)[1]
}
# Plotting the cross validation error
plot(c(1:50), CV_error, type = "l", xlab = "Values of q", ylab = "CV Error", main = "PLS CV Error")
which(CV_error == min(CV_error))
min(CV_error)
# Train logistic regression model with PCA components
CV_error <- rep(0, 50)
for (q in 1:50) {
prediction <- c()
for (i in 1:dim(XGtrain)[1]) {
GDATACV <- Gtrain[-i]
YDATACV <- as.data.frame(PCX$x[-i, 1:q])
suppressWarnings(logistic <- glm(GDATACV~., data = YDATACV, family = binomial(link = "logit")))
new_data <- as.data.frame(t(PCX$x[i, 1:q]))
colnames(new_data) <- colnames(YDATACV)
prediction[i] <- ifelse(predict(logistic, newdata = new_data) > 0, 1, 0)
}
CV_error[q] <- sum(prediction != Gtrain)/dim(XGtrain)[1]
}
# Plotting the cross validation error
plot(c(1:50), CV_error, type = "l", xlab = "Values of q", ylab = "CV Error", main = "PLS CV Error")
which(CV_error == min(CV_error))
min(CV_error)
# Retrain all the classifiers with the suggested value of q and the full training set
QDA_PLS <- qda(Gtrain~., data = as.data.frame(PLS$scores[, 1:15]))
QDA_PCA <- qda(Gtrain~., data = as.data.frame(PCX$x[, 1:6]))
logistic_PLS <- glm(Gtrain~., data = as.data.frame(PLS$scores[, 1:14]), family = binomial(link = "logit"))
logistic_PCA <- glm(Gtrain~., data = as.data.frame(PCX$x[, 1:4]), family = binomial(link = "logit"))
# Prepare the test set
Xtest <- XGtest[,-c(366)]
Gtest <- XGtest[, 366]
repbarX <- matrix(rep(colMeans(XGtrain), dim(Xtest)[1]), nrow = dim(Xtest)[1], byrow = T)
Y_new <- as.matrix(Xtest - repbarX) %*% gamma
# Report test error for quadratic discriminant with PCA components
PCA_newdata <- as.data.frame(Y_new[, 1:6])
prediction <- predict(QDA_PCA, newdata = PCA_newdata)$class
QDA_PCA_ERROR <- sum(prediction != Gtest)/dim(Xtest)[1]
paste("Test error for quadratic discriminant with PCA components is", QDA_PCA_ERROR)
# Report test error for logistic regression with PCA components
PCA_newdata <- as.data.frame(Y_new[, 1:4])
prediction <- ifelse(predict(logistic_PCA, newdata = PCA_newdata) > 0, 1, 0)
LR_PCA_ERROR <- sum(prediction != Gtest)/dim(Xtest)[1]
paste("Test error for logistic regression with PCA components is", LR_PCA_ERROR)
# Report test error for quadratic discriminant with PLS components
t_new <- as.matrix(Xtest - repbarX) %*% phi
PLS_newdata <- as.data.frame(t_new[, 1:15])
prediction <- predict(QDA_PLS, newdata = PLS_newdata)$class
QDA_PLS_ERROR <- sum(prediction != Gtest)/dim(Xtest)[1]
paste("Test error for quadratic discriminant with PLS components is", QDA_PLS_ERROR)
# Report test error for logistic regression with PLS components
PLS_newdata <- as.data.frame(t_new[, 1:14])
prediction <- ifelse(predict(logistic_PLS, newdata = PLS_newdata) > 0, 1, 0)
LR_PLS_ERROR <- sum(prediction != Gtest)/dim(Xtest)[1]
paste("Test error for logistic regression with PLS components is", LR_PLS_ERROR)
suppressMessages(library(randomForest))
# set.seed(90139)
rainfall.rf <- randomForest(factor(G)~., data = XGtrain, importance = TRUE, ntree = 1000)
OOB.error <- rainfall.rf$err.rate[,1]
plot(OOB.error, type = "l")
abline(h = mean(OOB.error), col = 'red', xlab = "number of trees used", lty = 2)
title("OOB error against the number of trees used")
legend("topright", c('OOB.Error','Mean'), col=c('black','4'), lty=c(1,2), inset=.01)
which(OOB.error == OOB.error[500])
# Refit the random forest classifier using the chosen number of tree
rainfall.rf <- randomForest(factor(G)~., data = XGtrain, importance = TRUE, ntree = 211)
fig.dim = c(8, 10)
varImpPlot(rainfall.rf)
rainfall.rf$importance[order(rainfall.rf$importance[,3], decreasing = TRUE)[1:30], 3:4]
test_prediction <- predict(rainfall.rf, XGtest[,1:365])
RF_ERROR <- sum(test_prediction != XGtest$G) / length(XGtest)
paste("Classification error:", RF_ERROR)
# Try refitting and running the tree 10 times
for (i in 1:10) {
rainfall.rf <- randomForest(factor(G)~., data = XGtrain, importance = TRUE, ntree = 435)
test_prediction <- predict(rainfall.rf, XGtest[,1:365])
RF_ERROR <- sum(test_prediction != XGtest$G) / length(XGtest)
print(paste("Classification error:", RF_ERROR))
}
rbind(LR_PCA_ERROR, LR_PLS_ERROR, QDA_PCA_ERROR, QDA_PLS_ERROR, RF_ERROR)
suppressMessages(library(randomForest))
# set.seed(90139)
rainfall.rf <- randomForest(factor(G) ~., data = XGtrain, importance = TRUE, ntree = 1000)
OOB.error <- rainfall.rf$err.rate[,1]
plot(OOB.error, type = "l")
abline(h = mean(OOB.error), col = 'red', xlab = "number of trees used", lty = 2)
title("OOB error against the number of trees used")
legend("topright", c('OOB.Error','Mean'), col=c('black','4'), lty=c(1,2), inset=.01)
which(OOB.error == OOB.error[500])
suppressMessages(library(randomForest))
# set.seed(90139)
rainfall.rf <- randomForest(factor(G) ~., data = XGtrain, importance = TRUE, ntree = 1000)
OOB.error <- rainfall.rf$err.rate[,1]
plot(OOB.error, type = "l")
abline(h = mean(OOB.error), col = 'red', xlab = "number of trees used", lty = 2)
title("OOB error against the number of trees used")
legend("topright", c('OOB.Error','Mean'), col=c('black','4'), lty=c(1,2), inset=.01)
which(OOB.error == OOB.error[500])
# Refit the random forest classifier using the chosen number of tree
rainfall.rf <- randomForest(factor(G)~., data = XGtrain, importance = TRUE, ntree = 223)
fig.dim = c(8, 10)
varImpPlot(rainfall.rf)
varImpPlot(rainfall.rf)
rainfall.rf$importance[order(rainfall.rf$importance[,3], decreasing = TRUE)[1:30], 3:4]
suppressMessages(library(randomForest))
set.seed(90139)
rainfall.rf <- randomForest(factor(G) ~., data = XGtrain, importance = TRUE, ntree = 1000)
OOB.error <- rainfall.rf$err.rate[,1]
plot(OOB.error, type = "l")
abline(h = mean(OOB.error), col = 'red', xlab = "number of trees used", lty = 2)
title("OOB error against the number of trees used")
legend("topright", c('OOB.Error','Mean'), col=c('black','4'), lty=c(1,2), inset=.01)
which(OOB.error == OOB.error[500])
# Refit the random forest classifier using the chosen number of tree
rainfall.rf <- randomForest(factor(G)~., data = XGtrain, importance = TRUE, ntree = 211)
varImpPlot(rainfall.rf)
test_prediction <- predict(rainfall.rf, XGtest[,1:365])
RF_ERROR <- sum(test_prediction != XGtest$G) / length(XGtest)
paste("Classification error:", RF_ERROR)
# Try refitting and running the tree 10 times
for (i in 1:10) {
rainfall.rf <- randomForest(factor(G)~., data = XGtrain, importance = TRUE, ntree = 435)
test_prediction <- predict(rainfall.rf, XGtest[,1:365])
RF_ERROR <- sum(test_prediction != XGtest$G) / length(XGtest)
print(paste("Classification error:", RF_ERROR))
}
rbind(LR_PCA_ERROR, LR_PLS_ERROR, QDA_PCA_ERROR, QDA_PLS_ERROR, RF_ERROR)
# Refit the random forest classifier using the chosen number of tree
rainfall.rf <- randomForest(factor(G)~., data = XGtrain, importance = TRUE, ntree = 211)
varImpPlot(rainfall.rf)
test_prediction <- predict(rainfall.rf, XGtest[,1:365])
RF_ERROR <- sum(test_prediction != XGtest$G) / length(XGtest)
paste("Classification error:", RF_ERROR)
test_prediction <- predict(rainfall.rf, XGtest[,1:365])
RF_ERROR <- sum(test_prediction != XGtest$G) / length(XGtest)
paste("Classification error:", RF_ERROR)
rbind(LR_PCA_ERROR, LR_PLS_ERROR, QDA_PCA_ERROR, QDA_PLS_ERROR, RF_ERROR)
# Refit the random forest classifier using the chosen number of tree
rainfall.rf <- randomForest(factor(G)~., data = XGtrain, importance = TRUE, ntree = 211)
test_prediction <- predict(rainfall.rf, XGtest[,1:365])
RF_ERROR <- sum(test_prediction != XGtest$G) / length(XGtest)
paste("Classification error:", RF_ERROR)
# Try refitting and running the tree 10 times
for (i in 1:10) {
rainfall.rf <- randomForest(factor(G)~., data = XGtrain, importance = TRUE, ntree = 211)
test_prediction <- predict(rainfall.rf, XGtest[,1:365])
RF_ERROR <- sum(test_prediction != XGtest$G) / length(XGtest)
print(paste("Classification error:", RF_ERROR))
}
library(MASS)
library(pls)
XGtrain <- read.table(file = "XGtrainRain.txt", header = TRUE, sep = ",")
XGtest <- read.table(file = "XGtestRain.txt", header = TRUE, sep = ",")
# QDA <- qda(G~.,data=XGtrain)
# summary(QDA)
LG <- glm(G~.,data=XGtrain,family=binomial(link = "logit"))
summary(LG)
Xtrain <- scale(XGtrain[,1:365], scale=FALSE)
PCX <- prcomp(Xtrain)
gamma <- PCX$rotation
n = nrow(Xtrain)
X <- as.matrix(Xtrain)
PCs <- (X-matrix(rep(1,n), nrow=n)%*%colMeans(X))%*%gamma
all(PCs == PCX$x)
PLS <- plsr(G~., data = XGtrain)
phi <- PLS$projection
T.matrix <- Xtrain%*%phi
all(T.matrix == PLS$scores)
G_train <- XGtrain[,366]
n = dim(X)[1]
CV=rep(0, 50)
for(k in 1:50) {
pred_list = rep()
for(i in 1:n) {
DATACV = as.data.frame(PCs[-i,1:k])
G_train_CV = G_train[-i]
QDA <- qda(G_train_CV~.,data=DATACV)
new <- as.data.frame(t(PCs[i,1:k]))
colnames(new) <- colnames(DATACV)
pred <- as.numeric(predict(QDA, newdata = new)$class)-1
pred_list[i] = pred
}
CV[k] = sum(pred_list != G_train)/n
}
plot(rep(1:50), CV, type = 'line')
(qCV=which(CV==min(CV)))
(min(CV))
G_train <- XGtrain[,366]
n = dim(X)[1]
CV=rep(0, 50)
for(k in 1:50) {
pred_list = rep()
for(i in 1:n) {
DATACV = as.data.frame(T.matrix[-i,1:k])
G_train_CV = G_train[-i]
QDA <- qda(G_train_CV~.,data=DATACV)
new <- as.data.frame(t(T.matrix[i,1:k]))
colnames(new) <- colnames(DATACV)
pred <- as.numeric(predict(QDA, newdata = new)$class)-1
pred_list[i] = pred
}
CV[k] = sum(pred_list != G_train)/n
}
plot(rep(1:50), CV, type = 'line')
(qCV=which(CV==min(CV)))
(min(CV))
G_train <- XGtrain[,366]
n = dim(X)[1]
CV=rep(0, 50)
for(k in 1:50) {
pred_list = rep()
for(i in 1:n) {
DATACV = as.data.frame(PCs[-i,1:k])
G_train_CV = G_train[-i]
LG <- glm(G_train_CV~.,data=DATACV,family=binomial(link = "logit"))
new <- as.data.frame(t(PCs[i,1:k]))
colnames(new) <- colnames(DATACV)
if (predict(LG, newdata = new) > 0) {
pred = 1
} else {
pred = 0
}
pred_list[i] = pred
}
CV[k] = sum(pred_list != G_train)/n
}
plot(rep(1:50), CV, type = 'line')
(qCV=which(CV==min(CV)))
(min(CV))
G_train <- XGtrain[,366]
n = dim(X)[1]
CV=rep(0, 50)
for(k in 1:50) {
pred_list = rep()
for(i in 1:n) {
DATACV = as.data.frame(T.matrix[-i,1:k])
G_train_CV = G_train[-i]
LG <- glm(G_train_CV~.,data=DATACV,family=binomial(link = "logit"))
new <- as.data.frame(t(T.matrix[i,1:k]))
colnames(new) <- colnames(DATACV)
if (predict(LG, newdata = new) > 0) {
pred = 1
} else {
pred = 0
}
pred_list[i] = pred
}
CV[k] = sum(pred_list != G_train)/n
}
plot(rep(1:50), CV, type = 'line')
(qCV=which(CV==min(CV)))
(min(CV))
Xtest <- as.matrix(sweep(XGtest[,1:365], 2, colMeans(XGtrain[,1:365])))
Gtest <- XGtest[, 366]
PCX <- Xtest%*%gamma
PC6s.test <- as.data.frame(PCX[, 1:6])
PC6s.train <- as.data.frame(PCs[,1:6])
QDA_PC6s <- qda(G_train~.,data=PC6s.train)
# QDA_PC6s <- qda(Gtest~.,data=PC6s.test)
pred <- predict(QDA_PC6s, newdata = PC6s.test)$class
table(pred, Gtest)
(QDA.PC.error <- sum(pred !=Gtest)/41)
# PLS <- plsr(G~., data = XGtest)
# phi <- PLS$projection
PLS15.test <- as.data.frame((Xtest%*%phi)[, 1:15])
PLS15.train <- as.data.frame(T.matrix[,1:15])
QDA_PLS15 <- qda(G_train~.,data=PLS15.train)
pred <- predict(QDA_PLS15, newdata = PLS15.test)$class
table(pred, Gtest)
(QDA.PLS.error <- sum(pred !=Gtest)/41)
PC6s.test <- as.data.frame(PCX[, 1:4])
PC6s.train <- as.data.frame(PCs[,1:4])
LG_PC6s <- glm(G_train~.,data=PC6s.train,family=binomial(link = "logit"))
Decision <- predict(LG_PC6s, newdata = PC6s.test)
pred[Decision<0]=0
pred[Decision>0]=1
table(pred, Gtest)
(LG.PC.error <- sum(pred !=Gtest)/41)
PLS <- plsr(G~., data = XGtest)
PLS14.test <- as.data.frame((Xtest%*%phi)[, 1:14])
PLS14.train <- as.data.frame(T.matrix[,1:14])
PLS14 <- glm(G_train~.,data=PLS14.train,family=binomial(link = "logit"))
Decision <- predict(PLS14, newdata = PLS14.test)
pred[Decision<0]=0
pred[Decision>0]=1
table(pred, Gtest)
(LG.PLS.error <- sum(pred !=Gtest)/41)
library(randomForest)
m.rf <- randomForest(formula=factor(G) ~. , data=XGtrain, ntree=3000, importance = TRUE)
OOB.error <- m.rf$err.rate[,1]
plot(OOB.error, type = "l")
abline(h = mean(OOB.error), col = 'blue', xlab = "number of trees used", lty = 2)
title("OOB error against the number of trees used")
legend("topright", c('OOB.Error','Mean'), col=c('black','4'),lty=c(1,2),inset=.01)
(B=which(OOB.error==OOB.error[3000]))
B = 220
m.rf <- randomForest(formula=factor(G) ~. , data=XGtrain, ntree=B, importance = TRUE)
varImpPlot(m.rf)
m.rf$importance[order(m.rf$importance[,3], decreasing = TRUE)[1:30], 3:4]
ClassPred=predict(m.rf,newdata=XGtest[,1:365])
(rf.error <- sum(ClassPred != XGtest$G)/41)
error.list <- c()
for (i in 1:20) {
rm <- randomForest(formula=factor(G) ~. , data=XGtrain, ntree=B)
ClassPred=predict(rm,newdata=XGtest[,1:365])
test.error <- sum(ClassPred != XGtest$G)/41
error.list <- c(error.list, test.error)
}
error.list
test.name <- c("QDA with PC", "QDA with PLS", "Logistic with PC", "Logistic with PLS", "Random Forest")
error.value <- c(QDA.PC.error, QDA.PLS.error, LG.PC.error, LG.PLS.error, rf.error)
error.matrix <- as.matrix(error.value)
row.names(error.matrix) <- test.name
error.matrix
test_prediction <- predict(rainfall.rf, XGtest[,1:365])
RF_ERROR <- sum(test_prediction != XGtest$G) / length(XGtest$G)
paste("Classification error:", RF_ERROR)
test_prediction <- predict(rainfall.rf, XGtest[,1:365])
RF_ERROR <- sum(test_prediction != XGtest$G) / length(XGtest$G)
paste("Classification error:", RF_ERROR)
# Refit the random forest classifier using the chosen number of tree
rainfall.rf <- randomForest(factor(G)~., data = XGtrain, importance = TRUE, ntree = 211)
varImpPlot(rainfall.rf)
test_prediction <- predict(rainfall.rf, XGtest[,1:365])
RF_ERROR <- sum(test_prediction != XGtest$G) / length(XGtest$G)
paste("Classification error:", RF_ERROR)
# Try refitting and running the tree 10 times
for (i in 1:10) {
rainfall.rf <- randomForest(factor(G)~., data = XGtrain, importance = TRUE, ntree = 211)
test_prediction <- predict(rainfall.rf, XGtest[,1:365])
RF_ERROR <- sum(test_prediction != XGtest$G) / length(XGtest$G)
print(paste("Classification error:", RF_ERROR))
}
rbind(LR_PCA_ERROR, LR_PLS_ERROR, QDA_PCA_ERROR, QDA_PLS_ERROR, RF_ERROR)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
suppressMessages(library(pls))
XGtrain <- read.table("XGtrainRain.txt", header = TRUE, sep = ",")
XGtest <- read.table("XGtestRain.txt", header = TRUE, sep = ",")
# Fitting the quadratic discriminant model
# qda_model <- qda(G~., data = XGtrain)
# Fitting the logistic regression model
logistic <- glm(G~., data = XGtrain, family = binomial(link = "logit"))
# summary(logistic)
# Obtain the projection matrix for PCA
Xtrain <- scale(XGtrain[, -c(366)], scale=FALSE)
PCX <- prcomp(Xtrain, retx = T)
gamma <- PCX$rotation
# Manually re-compute the PC components
Y <- (Xtrain - matrix(rep(1, nrow(Xtrain)), nrow=nrow(Xtrain)) %*% colMeans(Xtrain)) %*% gamma
all(PCX$x == Y)
# Obtain the projection matrix for PLS
PLS <- plsr(G~., data = XGtrain)
phi <- PLS$projection
t <- Xtrain %*% phi
all(PLS$scores == t)
phi
